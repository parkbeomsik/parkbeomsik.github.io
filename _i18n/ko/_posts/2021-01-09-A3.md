---
title: "A3: Accerelator for attention layer"
categories:
  - Microarchitecture
tags:
  - AI 가속기
  - Transformer
---

안녕하세요, 오늘은 Attention mechanism의 가속화에 대한 논문<sup>[1](#footnote_1)</sup>을 리뷰하겠습니다. 해당 논문은 2020 HPCA에 발표되었습니다.  

![ex_screenshot](https://drive.google.com/uc?export=view&id=1Ixf66K-QTCCFBBn12BwD5CEmcWq8j0Zv)<sup>[1](#footnote_1)</sup>

인공 신경망(Neural networks)이 많은 AI 문제를 해결할 수 있음이 밝혀짐에 다라, 이를 지원하기 위한 하드웨어 연구도 활발해지고 있습니다. 그 중 대표적인 것이 Diannao, Eyeriss 등인데요, 이들은 FPGA 또는 ASIC 기반의 디자인으로 CNN이나 RNN과 같은 망을 효율적으로 계산할 수 있었습니다. 하지만 최근 떠오르는 방식인 Attention mechanism을 지원하기 위한 하드웨어 디자인에 대해서 연구가 덜 되어있는데요, A3는 이 Attention mechanism을 가속화하는 방법에 대한 연구입니다.  

그 전에 먼저 Attention mechanism에 대해 알아보겠습니다. Attention mechanism은 자연어 처리 분야에서 많이 사용되는 기법으로, 여러 단어 혹은 여러 상태(state) 간의 관계를 학습에 반영하기 위한 목적을 가지고 있습니다.  

![ex_screenshot](https://drive.google.com/uc?export=view&id=1lZfV3KQORwcElY0WKL1N_DtKlasTzhOx)<sup>[1](#footnote_1)</sup>

Attention mechanism에는 Query, Key, Value 이렇게 세가지 종류의 벡터가 사용됩니다. 보통 Query vector는 하나, key와 value vector는 여러개가 사용됩니다. 첫번째로 먼저 Query vector와 각각 key vector의 유사도(similarity score)를 내적(dot-product)같은 방식으로 구합니다. 그리고 이를 softmax함수를 이용해 합계가 1이 되도록 스케일링 해준뒤, 그 결과를 가중치로 하여 Value vector들을 가중합(Weighted sum)해줍니다. 이렇게 나온 결과를 Attention score라 하며, 이 과정이 Attention mechanism입니다. 위 그림은 한 예로, Statements들이 Key vector, Query가 Query vector이며, 이 벡터들의 유사도를 이용해 Attention mechanism을 수행하는 모습입니다.  

![ex_screenshot](https://drive.google.com/uc?export=view&id=1HkAxHC1H1eG3oikAiO60qJZ3zO3ScwOH)<sup>[1](#footnote_1)</sup>

A3의 연구진은 먼저 Attention mechanism을 사용하는 3가지 모델들에 대해서 characterizaion을 수행했습니다. 그 결과 이들은 Attention mechanism이 3가지 모델에서 모두 연산 시간 중 많은 부분(약 35%)을 차지함을 발견했습니다. 또한, 전체 추론 중 많은 시간이 이미 제공된 지식들을 통계내는 데 많은 시간을 할애하고, 이는 Query 입력값이 들어오기 전에 처리할 수 있다는 점에 착안하여 이를 제외한 실제 question-answering task를 수행하는 시간에 대해서 characterizaion을 수행했습니다. 그 결과, Attention mechanism이 전체 연산 시간 중 차지하는 비율이 더 높아짐을 확인했습니다.  

A3는 이런 Attention mechanism을 효율적으로 계산하기 위한 가속기인데요, 이들은 병렬성(parallelism)을 극대화하기 위한 datapath를 설계하고 이를 위한 pipeline을 디자인했습니다. A3는 크게 3개의 모듈로 구성되어 있는데요, 이들은 Query와 Key의 유사도를 구하기 위한 `Dot-Product`모듈, Softmax함수 구현을 위한 `Exponent Computation`모듈, 그리고 Value의 가중합을 위한 `Output Computation`모듈입니다.

![ex_screenshot](https://drive.google.com/uc?export=view&id=1dp6XdfDGwb3iFHE3p5OM-aKeTspG9j7y)<sup>[1](#footnote_1)</sup>

먼저 첫번째 모듈에서는 Key vector와 query vector간의 내적을 계산합니다. 이때, 각 element들은 병렬적으로 계산하기 위해 여러개의 multipliers와 adder tree를 사용합니다. 또한 이렇게 구한 유사도 중 최대값을 구하는데요, 이 최대값은 다음 모듈에서 사용됩니다.

두번째 모듈에서는 softmax함수를 구현하기 위해 Exponent를 계산합니다. 그리고 exponent를 빠르게 계산하기 위해 lookup table 방식을 사용합니다. 이 때, lookup table 방식은 두 가지 문제점이 있는데요, 첫번째로 exponent function의 결과값이 input이 증가함에 따라 너무 빠르게 증가하여 overflow를 일으키기 쉽고, 두번재로 lookup table의 크기가 너무 커진다는 것입니다. 먼저 첫번째 문제를 해결하기 위해 이전 모듈에서 구했던 유사도의 최대값을 이용합니다. softmax 함수는 각 element에 상수항을 더하거나 빼도 결과가 불변하기 때문에 모든 element들에서 element 중 최대값을 빼주는데요, 이렇게 하면 모든 element가 0 이하로 맞춰지기 때문에 exponent function의 결과값이 1 이하가 됩니다. 그리고 두번째 문제를 해결하기 위해서 그들은 하나의 exponent operation이 두 개의 exponent operation으로 분리될 수 있음을 이용합니다. 따라서 두 개의 작은 lookup table만으로 하나의 큰 lookup table을 가지고 있는 효과를 낼 수 있습니다.  

마지막으로 attention score를 계산합니다. 이들은 앞서 계산한 score vector의 각 element들을 score의 총합으로 나누어주고(normalization), 이를 scaling factor로 하여 Value vector들을 가중합(Weighted sum)합니다. 그리고 가중합된 결과는 `output` register에 저장되고, 이 값이 최종 Attention score가 됩니다.  

다음으로 연구진들은 Attention mechanism을 더 효율적으로 계산하기 위해 Approximation 방법을 제안합니다. 주된 아이디어는 모든 Value vector들이 Attention score에 많은 영향을 미치지는 않는다는 것입니다. 앞서 Query와 Key의 유사도를 계산할 때, 0에 가까운 값이 많이 나오는데요, 이런 값들은 굳이 계산하지 않고 0으로 근사해도 전체 성능에 큰 영향을 미치지 않음을 알 수 있었습니다. 따라서, 이런 값들을 0으로 근사하고 Value를 가중합할 때 계산하지 않으면 Latency와 Power를 줄일 수 있을 것입니다.  

![ex_screenshot](https://drive.google.com/uc?export=view&id=1rhiwFLmeYKkuEKYE7Y4Upa3EoEXYI4RF)<sup>[1](#footnote_1)</sup>

하지만, 이 방법을 사용하더라도 전체 Key들에 대한 유사도는 계산해야 하는데요, 이들은 유사도를 끝까지 계산하지 않고도 그 Key vector가 score에 많은 영향을 미칠지 아닐지 구하는 greedy algorithm을 제안합니다. 위 그림에서와 같이 먼저 Key Matrix(Key vector들의 집합)와 Query matrix를 Element별로 곱셈해주고, 이렇게 나온 결과 중 최대값들과 최솟값들을 찾습니다. 그리고 이 값들만 더하여 Greedy score로 사용하면 최종 True score와 얼추 비슷한 경향을 보임을 알 수 있습니다.  

![ex_screenshot](https://drive.google.com/uc?export=view&id=1pa9P621TBh-pK7A5G40C28Xsvl4EvWRm)<sup>[1](#footnote_1)</sup>

이 Greedy search의 장점은 유사도를 끝까지 계산하지 않아도 되는 점도 있지만, Key Matrix에서 Column별로 정렬(sort)해놓으면 Greedy score를 산출하는 과정을 훨씬 단축시킬 수 있다는 점에 있습니다. 위 그림과 같이 먼저 Query vector가 양수인 index는 최댓값을, 음수인 index는 최솟값을 Key Matrix의 각 Column에서 뽑습니다. 그리고, 이를 Query element와 곱한 값을 score로 하고 rowID와 colID와 함께 maxQ(Priority queue)에 넣습니다. 이렇게 하면 maxQ 중 최댓값이 Key matrix와 Query matrix를 element-wise multiplication했을 때 각 element 중 최댓값과 같을 것입니다. 그리고 maxQ에서 최댓값으로 뽑힌 Key Matrix의 column에서 다음 최댓값(혹은 최솟값)을 뽑아 Query element와 곱한 뒤 maxQ에 넣어주면 Greedy search를 빠르게 수행할 수 있습니다.  

![ex_screenshot](https://drive.google.com/uc?export=view&id=1sdmlaRI-Qidzj6zSVIicA4UPeP1QbjH4)<sup>[1](#footnote_1)</sup>

연구진은 이 Approximation을 지원하는 Microarchitecture도 디자인했는데요, 위 그림과 같습니다. 먼저 앞서 보았든 미리 sorting된 Key matrix가 가운데에 저장되어 있고, 이전에서 MaxQ(혹은 MinQ)에서 최댓값(혹은 최솟값)으로 뽑힌 column index를 이용해 값을 가져옵니다. 그리고 이를 Query element와 곱한 뒤 Component mulplication buffer에 넣어줍니다. 그리고 이 Component mulplication buffer에서 최댓값 혹은 최솟값을 찾아 Greedy score에 합산해줍니다.  

Evaluation에서는 페이스북의 End-to-End Memory Network (MemN2N), Key-Value Memory Network (KV-MemN2N), 그리고 구글의 BERT 모델이 사용되었습니다. 또한, task로는 각각 모두 QA task인 bAbI QA task, Wiki-movies question-answering task, SQuAD v1.1 task를 사용했습니다.  

![ex_screenshot](https://drive.google.com/uc?export=view&id=1A9Uy4Dx8CKTeip10Zm2JOHQHgeV_Uiwe)<sup>[1](#footnote_1)</sup>

먼저 Approximation 방식에 대한 End-to-End accuracy를 계산했는데요, conservative한 approximation을 사용할 때는, 전체 accuracy가 크게 떨어지지 않음을 볼 수 있습니다. 다만, Aggressive한 approximation을 사용하면 accuracy가 꽤 떨어짐을 볼 수 있는데요, 연구진들은 이에 대해 Aggressive한 approximation을 사용할 때는 speedup과 energy efficiency 증가 효과로 인해 더 큰 모델을 사용할 수 있고, 이를 통해 더 좋은 정확도를 얻을 수도 있을 거라고 예상합니다.  

![ex_screenshot](https://drive.google.com/uc?export=view&id=1vTGynZfJ6y1UjnuAQjPhmMku8AX65RU-)<sup>[1](#footnote_1)</sup>

또한, A3 architecture를 사용했을 때, Throughput과 Latency를 구해보았는데요, 그 결과 CPU(Intel Xeon Gold 6128)나 GPU(NVIDIA Titan V)를 사용했을 때보다 훨씬 좋은 성능을 보여줌을 확인 할 수 있었습니다.

<a name="footnote_1">1</a>: Ham, Tae Jun et al. “A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation.” 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) (2020): 328-341.